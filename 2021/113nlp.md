# 自然言語処理にむけて

画像認識は、もともと画像データが多次元ベクトルとして、よく似た画像が近いベクトルで表現されるなど、機械学習で処理しやすい前提が整っていました。
一方、自然言語などのテキストは、文字コードが近くても意味が近いわけではありません。

自然言語を機械学習で処理するためには、テキストの特徴量を多次元ベクトルでうまく表現することが鍵になります。

<img src="https://stg2-cdn.go2senkyo.com/articles/wp-content/uploads/2015/07/0ec9a44365144d96fb771d83b7bcc02e.jpg" width="40%"/> 

AI入門のまとめとして、自然言語を機械学習で扱う方法をみていきます。

__モジュールの準備__

```py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
try:
    import japanize_matplotlib #matplotlibの日本語化  
except ModuleNotFoundError:
    !pip install japanize_matplotlib
    import japanize_matplotlib 
sns.set(font="IPAexGothic") #日本語フォント設定

```
## 形態素解析

言語における意味の基本単位は **語 (word) ** です。まず、語を取り出す方法からみていきましょう。

### 英語と日本語

自然言語処理は、言語の種類によって難しさや扱い方が異なります。

* (英語文) I bought a book 
* (日本語文) 私は本を買った

英語は、空白で区切られたものを語と考えることができます。したがって、Python の標準文字列ライブラリだけで、簡単に語を取り出すことができます。

__英語の字句解析__

```py
s = "I bought a book"
s.split()
```

日本語では、まず語の区切りを判定する必要があります。しかし、この語の区切りを判定するのが**かなりの難処理**となります。そこで、専用のツール/ライブラリが必要となります。

<div class="admonition note">

**形態素解析ツール/ライブラリ**

日本語文から、語の区切りを決定するツール/ライブラリ

</div>

### spaCy/GINZA

spaCy は、Explosion AI 社の開発するオープンソースの自然言語処理ライブラリです。2019 年に、 リクルート AI 研究所と国立言語研究所の研究成果である [GiNZA](https://megagonlabs.github.io/ginza/) が登場し、実用的な日本語処理が手軽に利用できるようになりました。

まずは、GiNZA の導入から始めましょう。`pip install ginza` を入力するだけで、 形態素解析も含め、自然言語処理に必要なライブラリがまとめてインストールされます。

```py
!pip install ginza
import pkg_resources, imp
imp.reload(pkg_resources)
```

SpaCy/GINZAは、各種自然言語処理をパイプライン化したツールになっています。パイプラインを調整することで、最終的な処理を切り替えることができます。

日本語文を形態素解析したいときは、次の通り、使います。

```py
import spacy
nlp = spacy.load('ja_ginza') 

doc = nlp("私は本を買った") #形態素解析

for word in doc:
    print(word.i, word.orth_, word.lemma_, word.pos_, word.tag_)
```

形態素解析によって得られた各字句(word)は、次のようなプロパティ情報をもっています。この中から必要な情報を取り出して利用します。

|プロパティ|情報|
|--|--|
|`orth_`|入力語|
|`lemma_`|原型|
|`pos_`|品詞(Part of Speech)|
|`tag_`|品詞タグ|
|`vector`|単語ベクトル|

日本語文を単語単位に分割する関数`wakachi(s)`を定義しておきましょう。

```py
def wakachi(s):
    doc = nlp(s)
    return [word.lemma_ for word in doc]  # word.lemma_ は標準形

print(wakachi('私は本を買った'))

```
<div class="admonition note">

(時間があったら）Let's try

自然言語処理と形態素解析の良い練習問題は、「自然言語処理１００本ノック」にあります。

http://www.cl.ecei.tohoku.ac.jp/nlp100/

Web上には、解説記事がたくさん掲載されていますので、参考にしながら解いてみると実力がつきます。

</div>

## アンケート分析

不動産屋による「まちづくりに関するアンケート」に基づいて、アンケート解析を試していきましょう。

__データの入手__

本データは、下山らによる「[Python 実践データ分析 100 本ノック](https://www.shuwasystem.co.jp/book/9784798058757.html)」から講義用に編集したものを利用します。

```
!wget https://KuramitsuLab.github.io/data/survey.csv
```


### コメントを眺める 

今まで、様々なデータを扱ってきましたが、
今回のデータの特徴は、自由記述形式のコメント、
満足度(5段階評価: 1 不満 - 5 満足)などが含まれています。

```py
data = pd.read_csv("survey.csv")
data.head() #最初の5行を表示
```

まず、アンケート中のコメントの分量を把握してみましょう。文字数を数えて、新しいカラム(`文字数`)を作って格納します。

```py
data["文字数"] = data["コメント"].str.len()
data.head()
```

```py
plt.hist(data["文字数"])
```

異常に長いコメントもありますね。コメントの文字数順に並べて直してみてみましょう。

__コメントの長さ順に並べ変えてみる__

```py
data.sort_values(by="文字数")
```

### 単語レベルの解析

コメントから単語を抽出して、どのような単語が使われているか調べてみましょう。

ここでは、形態素解析の品詞情報(`_pos`)から、動詞、形容詞、名詞だけに着目してみます。

```py
words = []
for text in data["コメント"]:
    doc = nlp(text)
    for word in doc:
        # 動詞(VERB), 名詞(NOUN), 形容詞(ADJ)のみ抽出
        if word.pos_ == 'VERB' or word.pos_ == 'NOUN' or word.pos_ == 'ADJ':
            words.append(word.lemma_)  #標準形に変換する
print(len(words))
print(words[:30]) #先頭30語だけ
```

これで、コメント文の中で用いられている名詞、動詞、形容詞をすべて取り出すことができました。

<div class="admonition note">
単語を抽出するコツ

* 標準形変換: 活用のある単語（例. 「買った」）は、買うのように標準形に変換する

* [ストップワード除外](https://mieruca-ai.com/ai/nlp-stopwords/#toc_2-1): 解析の精度を上げるために不要な記号や単語を取り除く

</div>

### 単語の頻出度と視覚化

単語の抽出ができたら、最初のステップとして、単語の出現頻出度から調べていきます。

ここは、新しいデータフレームを作り、`value_counts()`で調べてみましょう。

```py
pd.DataFrame({"words":words}).value_counts()
```

ワードクラウドは、テキストデータを視覚的に表現する方法です。ちょっと寄り道をして表示してみましょう。

まず、日本語フォントをインストールしておきます。（これがないと文字化けします。）

__Colab上でのIPA日本語フォントのインストール__
```
!apt-get -y install fonts-ipafont-gothic
```

```py
## !pip install wordcloud

from wordcloud import WordCloud
fpath = '/usr/share/fonts/truetype/fonts-japanese-gothic.ttf'
#fpath = 'fonts-japanese-gothic.ttf'
word_chain = ' '.join(words)

model = WordCloud(width=800, height=600, background_color='white', colormap='bone', font_path=fpath)
W = model.generate(' '.join(words))

plt.imshow(W)
plt.axis('off')
plt.show()
```

アンケートの満足度に影響を与えているキーワードが見えてきました。しかし、まだどのキーワードがプラスの評価なのか、マイナスの評価なのかわかりません。

<div class="admonition note">

Let's try

満足度の高いキーワードを抽出してみよう

</div>

### 満足度の高いキーワード

今回のアンケート調査の素晴らしいことは、不動産の満足度が 5 段階評価で回答されている点です。
各キーワードとこの5段階評価を紐付けてみると、キーワードの満足度が見えて来るかもしれません。 

コメント内の単語と満足度をペアにして取り出してみます。

```py
words = []
scores = []
for text, score in zip(data["コメント"], data["満足度"]):
    doc = nlp(text)
    for word in doc:
        if word.pos_ in ['VERB', 'NOUN', 'ADJ']:
            words.append(word.lemma_)
            scores.append(score)
print(words[:30])
print(scores[:30])
```

Pandas を使って表データにまとめておきましょう。`出現数`1回のカラムを作って、`groupby`してみましょう。満足度は平均値(`np.nean`)をとって、集計します。

```py
keyword = pd.DataFrame({"キーワード": words, "満足度": scores, "出現数": [1]*len(words)})
keyword.groupby('キーワード').agg({'満足度': np.mean, '出現数': sum}).sort_values(by='出現数')
```

<div class="admonition note">

Let's try

満足度の高いキーワードと満足度の低いキーワードのトップ 5を出してみよう

</div>

今回の分析は、出現頻度のあまりに低い単語を除外した方が良いです。このように、データサイエンティスト(分析者)のセンスで、結果は少し変わります。


<div class="admonition note">

極性辞書とセンティメント解析

極性辞書は、ある単語が一般的にネガティブなのか、ポジティブなのかを、-1（ネガティブ）から1（ポジティブ）までのスコアの形で表現したものです。

* [日本語評価極性辞書](http://www.cl.ecei.tohoku.ac.jp/Open%20Resources_Japanese%20Sentiment%20Polarity%20Dictionary.html)：東北大の乾・岡崎研究室が公開
* [単語感情極性対応表](http://www.lr.pi.titech.ac.jp/~takamura/pndic_ja.html)：東工大の高村教授が公開

センティメント分析などに便利です。（今回の満足を極性辞書と比較してみましょう。）

</div>

## 文章のベクトル化

次は、いよいよ文章のベクトル化を考えていきます。

ポイントは、意味や内容が似ている文が近くなるようにベクトル化することです。もともと、類似文章検索として研究されてきました技術になります。

まず、準備として、`わかち書き`されたカラムを作っておきましょう。

```py
data['わかち書き'] = data['コメント'].map(lambda x: ' '.join(wakachi(x)))
data.head()
```

### BOW

**BOW(Bag of Words)** は最も古典的な文書の特徴量を捉えてベクトル化する手法です。
出現する単語の個数を $N$ とすると、各コメント文は出現した単語には 1 を入れた $N$ 次元のベクトルで表現します。

BOW のポイントは、文章の構造は全て無視し、「どの単語が含まれているか」だけに注目している点です。そして、一旦、コメント文をベクトルで表現できれば、**コサイン類似度 (cosine similarity)** を用いて、類似度を求めることができます。

BOW の原理は、難しくありません。sklearnモジュールの`CountVectorizer`を使って、楽に BOW を求めることができます。ただし、sklearn は、英語圏で開発されたライブラリなので、入力文は英単語のように空白で区切られているという前提になっています。日本語は、形態素解析を使って前処理して、テキストを空白区切りの形式に変換しておく必要があります。

```py
from sklearn.feature_extraction.text import CountVectorizer

docs = np.array(data['わかち書き'])
model = CountVectorizer()
bags = model.fit_transform(docs)

print(bags.toarray())
```

Pandasで表データとマージして、コメントがどのようにベクトル化されたかみてみましょう。

```py
pd.DataFrame(bags.toarray(),columns=model.get_feature_names(),index=data['コメント']).head()
```

BOWは、単語の並びを無視しています。語順を無視すると重要な情報が飛んでしまいそうですが、不思議なことに類似文書検索では、十分精度がでます。

### コサイン類似度

コサイン類似度は、文書ベクトルの類似度を測る尺度としてよく使われます。
ベクトルの向きがどの程度同じ方向を向いているか？という指標で、$-1$～$1$の範囲をとります。

コサイン類似度を数式で記述すると以下のようになります。

$$
cos(\mathbf{x}, \mathbf{y}) 
= \frac{\mathbf{x} \cdot \mathbf{y}}{|\mathbf{x}| \cdot|\mathbf{y}|} 
= \frac{\sum_{i=1}^{|V|} x_i y_i}{\sqrt{\sum_{i=1}^{|V|} x_i^2} 
    \cdot \sqrt{\sum_{i=1}^{|V|} y_i^2}}
$$

__なんかそろそろNumPyの方が読みやすくなってきましたね__

```py
def cosine_similarity(x, y):
    return np.dot(x, y)/(np.sqrt(np.dot(x, x))*np.sqrt(np.dot(y, y)))

X = np.array([0.7, 0.5, 0.3,0.1])
Y = np.array([0.8, 0.5, 0.2, 0.222])
print(cosine_similarity(X, Y))
```

もちろん、skleranモジュールにもコサイン類似度のライブラリは含まれています。
こちらは、ユニバーサル関数バージョンになっているので気をつけましょう。

```py
from sklearn.metrics.pairwise import cosine_similarity
X = np.array([[0.7,0.5,0.3,0.1], [0.1,0.2,0.9,0.9]])
Y = np.array([[0.8, 0.5, 0.2, 0.2], [0.1,0.2,0.9,0.9]])

print(cosine_similarity(X, Y))
```

実際に、「スポーツできる場所があるのが良い(index=1)」と類似しているコメントを探してみましょう。

```py
def print_sim(index):
    vec = bags.toarray()
    similarity = cosine_similarity(vec[index:index+1], vec)[0]
    top10 = np.argsort(similarity)[::-1][:10]
    for i in top10:
        print(similarity[i], data['コメント'][i])
print_sim(1)
```

### TF/IDF

BOW は、単語の出現を見るだけで、重要度を考慮に入れていません。

TF-IDF(Term Frequency-Inverse Document Freequency: 単語頻度-逆文書頻度) は、
よくある一般的な単語と**特徴のある重要な単語**の区別をつける指標です。

__単語 w が n 回現われるとき、TF(Term Frequence)__
$$
TF = \frac{n}{N}
$$

__単語 w を含む文が d 個あるとき、IDF(Inverse Document Frequency)__
$$
IDF = - \log{\frac{d}{D}} = \log{\frac{D}{d}}
$$

TF-IDF は、$TF$ と $IDF$ の積によって求まる。

$$
\mbox{TF-IDF} = \frac{n}{N} \log{\frac{D}{d}}
$$

IDF は一種の一般語フィルタとして働き、多くの文書に出現する語(一般的な語)は重要度が下がり、
特定の文書にしか出現しない単語の重要度を上げる役割を果たします。


TF/IDF を用いることで、重要度の重みつけされたベクトルが得られます。
（本当は、自分で計算してみましょうとしたいのですが、）sklearn のライブラリを用いてお手軽に計算してみます。

```py
from sklearn.feature_extraction.text import TfidfVectorizer
# tf-idf

vectorizer = TfidfVectorizer(max_df=0.9) #文書全体の90%以上で出現する単語は無視する
X = vectorizer.fit_transform(data['わかち書き'])
print('feature_names:', vectorizer.get_feature_names())
print('X:')
print(X.toarray())

```

```py
pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names(),index=data['コメント']).head()
```

<div class="admonition note">
Let's try

TF/IDFの場合の類似コメントを探してみよう。

</div>

### LSA

LSA（潜在的意味解析）では、トピックという文書と単語の間に存在する抽象的な概念を導入し、各文書の BOW あるいは TF-IDF ベクトルを行とする文書数×単語数の行列を特異値分解（SVD)し、文書数×トピック数に次元削減します。 

__８次元に減らしてみます__

```py
from sklearn.decomposition import TruncatedSVD
np.set_printoptions(suppress=True)
# SVD
svd = TruncatedSVD(n_components=8, n_iter=7, random_state=0)
svd.fit(X.toarray())
X = svd.transform(X.toarray())

```

```py
pd.DataFrame(X,index=data['コメント'])
```

では、「スポーツできる場所があるのが良い(index=1)」と類似しているコメントをみてみましょう。

```py
def print_sim(index):
    similarity = cosine_similarity(X[index:index+1], X)[0]
    top10 = np.argsort(similarity)[::-1][:10]
    for i in top10:
        print(similarity[i], data['コメント'][i])
print_sim(1)
```

参考資料：
https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part1.html

## 単語分散表現

文書中の単語出現数を元に文書ベクトルを紹介してきましたが、最後に単語の持つ意味的な情報を用いる手法として、**単語分散表現**（単語ベクトル）について紹介します。

単語分散表現では、単語を多次元空間上の座標にマッピングすることで、単語同士の類似度を比較したり、加減算したりすることができるようになります。


単語分散表現は、さまざまな方法で求められます。
しかし、2013年にGoogle研究所が発表した Word2Vecが有名です。
これは、「同じ文脈で登場する単語は似た意味を持つ」という分布仮説をベースにして、ニューラルネットワークで計算されます。


### 単語ベクトル

GiNZAは、形態素解析したときに単語ベクトルが`voctor`プロパティで付与されています。

__単語ベクトルを確認してみる__

```py
doc = nlp('スポーツ 良い')
print(doc[0].vector.shape, doc[0].vector) # 「スポーツ」の単語ベクトル
#print(doc[1].vector.shape, doc[1].vector) # 「良い」の単語ベクトル
```

### 文章ベクトル

文章ベクトルは、単語ベクトルから算出されます。

この算出方法は色々あります。GiNZAでは、各単語の平均値ベクトルとして算出されるようです。
```py
doc = nlp('ポーツできる場所があるのが良い')
print(doc.vector.shape, doc.vector) # 単語ベクトル

```
このような文書ベクトルを使うことでも文書類似度検索を行うことができます。

<div class="admonition note">

Let's try

GiNZA の文書ベクトルを用いて、コサイン類似度から類似度検索をしてみよう。
TF/IDF と比較してみると面白いかも..

</div>

現在の自然言語処理では、TransformerによるBERTと呼ばれる文脈を含んだ
ベクトル化が発明されて、人工知能や機械翻訳の精度が大きく向上しています。
さらに詳しく勉強したい人は、一緒に研究しましょう。

## コースワーク

今回のアンケートでは、自由形式のコメントと満足度を同時に回答するようになっていたため、満 足度の高いキーワードを抽出できました。また、コメント文をベクトル化することで、コメント間 の類似度が求められることも見えてきました。

<div class="admonition tip">

**疑問（文書からの満足度）**

満足度は予測できるのでしょうか?

</div>

これは、エントリーシートから(採用後の)満足度は予想できるのでしょうか?と同じ質問になります。
皆さんは AI がエントリーシートを判定しているという噂を聞いたことがありますね。

<div class="admonition danger">

**演習（エントリシート）**

企業がどのように AI を活用して、エントリーシートを分析しているか考察してみよう。
（可能であれば、今回のコメント文から満足度を予測するモデルを構築してみよう。）

</div>

今まで学んできた知識を総動員して、もし足りなかったら追加で調査して考えてみましょう。


