{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ニューラル・ネットワーク\n",
    "\n",
    "いよいよ2010年代の人工知能・機械学習の立役者"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ニューラル・ネットワークの原理\n",
    "\n",
    "ニューラル・ネットワーク(neural network)は、人間の脳の構造を模した人工知能アルゴリズムです。\n",
    "\n",
    "### ニューロン\n",
    "\n",
    "人間の脳は、ニューロン(neuron)と呼ばれる神経細胞から構成されます。\n",
    "\n",
    "ニューロンを単純化した数理モデルで考えます。\n",
    "\n",
    "![neuron-fs8.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/57754/f4485c8d-5f40-e478-a95f-043071b17ba6.png)\n",
    "\n",
    "\n",
    "ネットワークの**重み**を$w_1, w_2$とすると：\n",
    "\n",
    "__ニューロンから伝わる信号の総量__\n",
    "\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2\n",
    "$$\n",
    "\n",
    "発火：次のニューロンに信号を伝える\n",
    "\n",
    "入力の信号量がある閾値$\\theta$を超えるかどうかで決まる\n",
    "\n",
    "* 発火　$(w_1 x_1 + w_2 x_2 \\ge \\theta)$\n",
    "* 発火しない　$ (w_1 x_1 + w_2 x_2 < \\theta)$\n",
    "\n",
    "__(学習のイメージ）誤り訂正学習法__\n",
    "\n",
    "ある組み合わせ$(w_1, w_2)$で入力を試し、\n",
    "出力が誤っていたらパラメータを調整することで徐々に正しい状態に近づける手法です。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 単純パーセプトロン\n",
    "\n",
    "単純パーセプトロンは、ニューラルネットワークの単純な数理モデルです。\n",
    "\n",
    "$$\n",
    "y = f(\\mathbf{w}\\cdot\\mathbf{x}+b)\n",
    "$$\n",
    "\n",
    "* 入力: $\\mathbf{x} = (x_1, x_2, ..., x_n)$\n",
    "* 重み: $\\mathbf{w} = (w_1, w_2, ..., w_n)$\n",
    "* バイアス: $b$\n",
    "* 活性化関数 $f(x) = \\begin{cases}\n",
    "    1 & (x>0) \\\\\n",
    "    0  & (x\\le0)\n",
    "  \\end{cases}$ \n",
    "\n",
    "### 確率モデルの導入\n",
    "\n",
    "活性化関数は、ニューロンの発火を定める関数です。\n",
    "前の説明では、ニューロンの発火を0,1で決定していましたが、\n",
    "機械学習ではきれいに0,1に分類されることはありません。\n",
    "発火しそうだけどギリギリ発火しないなどの中間的な状態があります。\n",
    "パーセプトロンでは、活性化関数として、\n",
    "ロジスティック回帰でも用いた標準シグモイド関数を使うことで、\n",
    "0から1の連続値を扱えるようになります。\n",
    "これで、0.49のようなギリギリ発火しない状態も表現できます。\n",
    "\n",
    "__標準シグモイド関数__\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "ちなみに、シグモイド関数が好まれる理由は、微分をしてみると、シグモイド関数の形で表されるからです。\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "シグモイド関数を用いると、確率的分類モデルになります。\n",
    "\n",
    "* ニューロンが発火する確率: $p(C = 1 ~|~ x) = \\sigma(\\mathbf{w} \\mathbf{x} + b)$\n",
    "* ニューロンが発火しない確率: $p(C = 0~|~ x) = 1 - p(C = 1 ~|~ x) = 1 - \\sigma(\\mathbf{w} \\mathbf{x} + b)$\n",
    "\n",
    "確率変数$C$は、0か1なので、$y = \\mathbf{w} \\mathbf{x} + b$として、\n",
    "次の式にまとめられます。\n",
    "\n",
    "$$\n",
    "p(C = t | x) = y^t(1-y)^{(1-t)}\n",
    "$$\n",
    "\n",
    "尤度関数(ゆうど）は、ある前提条件に従って結果が出現する場合に、逆に観察結果からみて前提条件が「何々であった」と推測する尤もらしさ（もっともらしさ）を表す数値を関数として捉えたものです。\n",
    "\n",
    "尤度関数: wとbを尤度推定するための関数\n",
    "\n",
    "$$\n",
    "L(w, b) = \\prod_{n=1}^{N} p(C = t_n|x_n) = \\prod_{n=1}^{N} y_n^{t_n}(1 - y_n)^{1-t_n}\n",
    "$$\n",
    "\n",
    "### ニューラルネットワークの学習\n",
    "\n",
    "ニューラル・ネットワークの学習は、\n",
    "尤度関数$L(w, b)$を最大化するように$w$と$b$を調整することになります。\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**最適化問題(optimization problem)**\n",
    "\n",
    "関数が最大・最小となる状態を求める問題のこと。\n",
    "関数の最大化は、符号を反転すると、最小化に置き換えられるので、\n",
    "一般に関数を最適化するとは、関数を最小化するパラメータを求めることです。\n",
    "</div>\n",
    "\n",
    "パラメータの偏微分（勾配）を求める。\n",
    "\n",
    "積の形をしているので、偏微分の計算が煩雑になります。そこで、事前の準備として、対数をとって、和の形に変形しておきます。\n",
    "\n",
    "__交差エントロピー誤差関数(cross-entropy error function)__\n",
    "$$\n",
    "E(w, b) = - \\log{L(w, b)} = - \\sum_{n=1}^{N} t_n \\log{y_n} + (1 - t_n)\\log{1-y_n}\n",
    "$$\n",
    "\n",
    "$E(w, b)$を最小化することがもともとの尤度関数の最適化になります。一般的には、$E$のことを**誤差関数(error function)**、もしくは、**損失関数(loss function)**と呼びます。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 勾配降下法\n",
    "\n",
    "交差エントロピー誤差関数$E(w,b)$を最適化するためには、$w, b$で偏微分して0になるパラメータを求めることになります。しかし、解析的にこの値を求めるのは困難な場合があります。\n",
    "そこで、パラメータを逐次的に更新することで、最適化を探索するアプローチがとられます。\n",
    "\n",
    "__勾配降下法(gradient descent)__\n",
    "\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\nu \\frac{\\partial E(w, b)}{\\partial w}　\n",
    "= w^{(k)} - \\nu \\sum_{n=1}^{N}(t_n - y_n)x_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{(k+1)} = b^{(k)} - \\nu \\frac{\\partial E(w, b)}{\\partial b} \n",
    "= b^{(k)} - \\nu \\sum_{n=1}^N (y_n - t_n)\n",
    "$$\n",
    "\n",
    "(直感的な解釈)： 予測値と実際の値との誤差(y_n - t_n)を用いて、パラメータが更新されます。つまり、ニューラルネットワークの目標は、「予測値と実際の値」の差をなくすことなので、直感に反しない解釈となります。\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "学習率(learning_rate): $\\nu(>0)$\n",
    "\n",
    "学習率は、収束しやすさを調整するハイパーパラメータです。\n",
    "通常は、$0.1$や$0.01$などの適当な小さい値を与えます。\n",
    "\n",
    "</div>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 単純パーセプトロンの実装\n",
    "\n",
    "NumPyを用いて単純パーセプトロンを実装してみましょう。\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DIM=2\n",
    "LR = 0.1 #学習率\n",
    "\n",
    "# モデルのパラメータ\n",
    "w = np.random.normal(size=(DIM,))\n",
    "b = 0\n",
    "\n",
    "def forward(y):\n",
    "    y = activate(np.matmul(w, x) + b)\n",
    "    return y\n",
    "\n",
    "def activate(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def compute_loss(t, y):\n",
    "    return (-t * np.log(y) - (1 - t)*np.log(1-y)).sum()\n",
    "\n",
    "def train_step(x, t):\n",
    "    y = forward(x)\n",
    "    delta = y - t\n",
    "    dw = np.matmul(x, delta)\n",
    "    db = np.matmul(np.ones(DIM), delta)\n",
    "    w = w - LR * dw\n",
    "    b = b - LR * db\n",
    "    loss = compute_loss(t, y)\n",
    "    return loss    \n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 多層パーセプトロン\n",
    "\n",
    "単純パーセプトロンは、原理的にロジスティック回帰と等価です。\n",
    "（こんなに頑張っても）線形分離可能な問題しか解くことができません。\n",
    "\n",
    "線形分離不可能な問題を解くためのアイディアは、パーセプトロンを組み合わせて多層化することです。\n",
    "\n",
    "### 深層学習へ\n",
    "\n",
    "多層ニューラルネットの学習は、４層以上の局所最適解や勾配消失などの技術的な問題によって、十分に学習させられず、性能も芳しくないとして、1990年代を中心とした時期には研究なども退潮気味にあった。\n",
    "\n",
    "機械学習において、ニューラルネットワークを使用した次元圧縮のためのアルゴリズム。2006年にジェフリー・ヒントンらが提案した。\n",
    "\n",
    "多層パーセプトロンに数々の工夫を加えたものが深層学習です。\n",
    "\n",
    "![deep1-fs8.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/57754/ebc23dd0-544b-7b72-185e-510a584aae8b.png)\n",
    "\n",
    "![deep2-fs8.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/57754/49663032-edde-3f61-c00b-cc1dcf5d0704.png)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## コースワーク\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Let's try\n",
    "\n",
    "`2 ** (1//2)` が、正しく $\\sqrt{2}$ にならない理由を考えてみよう\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "(プログラミングの)$N$個数える\n",
    "\n",
    "プログラミングでは、原則、「**0から**$N-1$まで」のように数えます。\n",
    "\n",
    "</div>\n",
    "\n",
    "### 平均点\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "\n",
    "**例題（平均点）**\n",
    "\n",
    "期末試験は5人受験した。\n",
    "点数が40点未満の生徒は全員，補習を受け，成績が40点になった。\n",
    "5人の平均点を求めよ。\n",
    "\n",
    "入力例：\n",
    "```\n",
    "10\n",
    "65\n",
    "100\n",
    "30\n",
    "95\n",
    "```\n",
    "\n",
    "出力例：\n",
    "```\n",
    "68\n",
    "```\n",
    "\n",
    "[AtCoder (JOI2014 予選)](https://atcoder.jp/contests/joi2014yo/tasks/joi2014yo_a)\n",
    "\n",
    "</div>\n",
    "\n",
    "__(解法) リストを使う場合__\n",
    "\n",
    "1. 期末試験を記録する空の得点リスト `scores` を用意する\n",
    "2. 5人分繰り返し、点数を読んで、`scores` に追加する \n",
    "3. 平均点は `sum(scores) // 5`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}